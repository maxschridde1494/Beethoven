## Configuration Details

Beethoven offers flexible configuration through environment variables (set in your `.env` file) and the MediaMTX YAML. Below is an overview of important configuration options and how to use them:

* **Roboflow API Configuration:** These settings tell the server how to reach the Roboflow inference service and which models to use. In the `.env`:

  * `ROBOFLOW_API_URL` – The URL of the Roboflow inference server. By default this is `https://detect.roboflow.com` (Roboflow's cloud endpoint). If you are using Roboflow's hosted service, keep this unchanged. If you have set up a **local Roboflow inference server** (for example, via a Docker container or on-premise service), you would change this to that server's address. The comment in the sample file suggests `http://host.docker.internal:9001` as an example for a local server (assuming you run the Roboflow container on your host at port 9001; `host.docker.internal` allows the container to reach your host machine).
  * `ROBOFLOW_API_KEY` – Your private API key for Roboflow. This **must** be provided for authentication when using the cloud API (and may also be needed for a local server depending on how Roboflow's local inference is set up). The key is usually a short string of letters/numbers you get from your Roboflow account dashboard. Keep this secret and do not commit it to source control.
  * `ROBOFLOW_MODEL_ID` – The identifier of the Roboflow **model used for main inference**, i.e. detecting pressed keys. It typically includes your workspace or project name and the model version number. In the sample it's set to `"beethoven-keys-pressed/22"` (which implies a model named "Beethoven Keys Pressed", version 22). You should set this to the model ID that detects **key presses** in your project.
  * `ROBOFLOW_RELATIVE_POSITION_MODEL_ID` – The identifier of the Roboflow **relative position model** that is used to find the piano keys' positions. In the sample it's `beethoven-relative-position/5`, corresponding to a model trained to detect the location of each piano key (distinguishing white vs black keys) on an image of a piano. This is run on the static reference image(s) once at startup for calibration. If you have your own model for this or if the ID is different in your Roboflow account, update this accordingly. If you choose not to perform an initial layout inference, you can leave this blank, and the server will skip the initial step (though skipping it means the system won't know which specific keys are where, which could limit functionality).

* **Camera Stream Settings:** The `CAM_PROXY_CONFIG` variable in the environment config defines what video streams to use and how to initialize them. This is one of the most important configurations to get right. It is a JSON array of objects, where each object has:

  * `name` – A short identifier for the camera or video feed. Choose a simple name (no spaces) as it will be used in URLs and references. For example: `"middle-left"` or `"camera1"`. This name should correspond to an entry in the MediaMTX config so that an HLS stream is available for it. In the default `mediamtx.yml`, **"middle-left"** and **"edge-left"** are pre-defined. If you use different names, you should modify `mediamtx.yml` accordingly (or add the new names).

  * `stream_url` – The location of the video source. This can be a file path or a network stream URL. If the value does **not** start with `rtsp://`, the system assumes it's a local file path. For example, `"app/assets/middle-left.mp4"` would refer to a video file (mounted in the server container). The server will launch FFmpeg to read this file and **publish** it to the stream proxy (MediaMTX) under the given `name`. It will also loop the video continuously (`-stream_loop -1` in FFmpeg) so the stream acts like a live source. If the value **does** start with `rtsp://`, the server treats it as a live RTSP feed (e.g., from an IP camera). In that case, the server will directly connect to that RTSP URL for analysis, and it expects that the stream proxy will also have that feed available for the client. You can achieve that either by having the server itself push the RTSP into MediaMTX (the server isn't currently set up to re-stream RTSP to RTSP, it usually pushes file sources) or by configuring MediaMTX to pull the camera's RTSP feed as described earlier. In summary: use file paths for local videos or use `rtsp://` URLs for live streams, and ensure MediaMTX is aware of those streams (for files, it's automatic; for direct RTSP, configure mediamtx or adapt as needed).

  * `relative-position-img-path` – Path to the static image file used for initial inference for that camera. For best results, this image should be a clear shot of the piano keys (from the same angle as the video) without hands or motion – essentially what the piano looks like at rest. The server will load this image at startup and run the `ROBOFLOW_RELATIVE_POSITION_MODEL_ID` detection on it to identify all the keys and their positions. From that, it can map out which key is which (e.g., "this bounding box corresponds to note C4, key number 40", etc.). The path can be relative to the `server/app` directory (like in the sample, which assumes images are in `app/assets/`). **Tip:** name your image file something like `<cameraName>-static.jpg` to keep it clear (the sample code actually tries to fetch `<name>-static.jpg` from the API static files route). After adding your images, you might need to adjust `server/app/main.py` to mount the `assets` folder for static serving if it isn't already (in case you want to serve images through the API).

  * **Frame Interval:** Another related setting is `INTERVAL`, which controls how often frames are sampled from the video for inference. By default `INTERVAL=1` (which effectively means every frame, if 1 is interpreted as one frame interval). You can increase this number to process frames less frequently (e.g., `2` to take every other frame, or a fractional value like `0.5` to process two frames per second if your video is 1 FPS, depending on implementation). The interval is in seconds between inference runs in the code, so if your video is 30 FPS and you set `INTERVAL=1`, it might infer roughly every second, not every frame – be mindful of how it's implemented if you adjust it. Tuning this can help with performance if needed.

* **Database Settings:** In most cases you won't need to change the database config for local use. The provided `DATABASE_URL` will point the server to the included Postgres container. However, if you want to connect to an external or existing Postgres instance, you can change this URL accordingly (just be sure the container can reach it, and adjust credentials). The format is `postgresql+psycopg2://username:password@host:port/dbname`. The variables `POSTGRES_USER`, `POSTGRES_PASSWORD`, etc., in the docker-compose are for initializing the internal DB and typically don't need changing. The main toggles you have are:

  * `DATABASE_PERSIST_DETECTIONS` – set this to `"true"` if you want to save detection events to the database. Each detection (each frame where the model finds one or more pressed keys) will be inserted into a table, along with info like timestamp, which key, confidence, etc. This can generate a lot of data over time, so if you're just testing or don't need that record, leave it as `"false"`. Even with it false, the database can still be used by other features, but currently its primary use is storing detections.
  * `DATABASE_ECHO` – set this to `"true"` to enable SQL query logging from SQLAlchemy (the ORM). This is mainly for debugging; it will print SQL statements to the server log. It's safe to leave false unless you are diagnosing a database issue or want to see the exact queries being run.

* **Logging Level:** You can control the verbosity of the server logs with the `LOG_LEVEL` variable. By default it's `INFO`, which prints informational messages, key events, and warnings/errors. If you need more detail (for example, debugging why something isn't working), you can set `LOG_LEVEL=DEBUG` to get very detailed logs. Conversely, you could set it to `WARNING` or `ERROR` to reduce output. The server's logging is configured via a custom logger utility and will respect this level across the app.

* **MediaMTX Config:** While not an environment variable, it's worth noting the role of **`server/mediamtx.yml`** in configuration. This file defines how the MediaMTX (stream proxy) operates. We've discussed it above, but to recap: the config included in the repo enables HLS streaming and sets up placeholder stream names expecting a publisher. If you add or change camera `name`s, update this file to have corresponding entries under `paths:`. If you want to ingest RTSP streams directly, put them here as `source` URLs (and comment out or remove the `source: publisher` entries for those names). The container automatically loads this config on startup (the compose file mounts it read-only into the container). Typically you won't need to change `mediamtx.yml` for initial testing with the default names – just ensure your `CAM_PROXY_CONFIG` names match those in the config. 